\section{Relates works}\label{sec:OVERVIEW}

In this section we will focus on there major areas related to our research. Firstly we will overview, how a problem of large scale multidimensional indexing is solved in industry right now. Then we will address a problem of instance-based learning and which techniques can be used to minimize dataset size, but preserve classification accuracy on the same level. Finally, we will discuss proximity graphs and their application to instance-based learning.

\subsection{Large-scale index structures}

Problem of large scale indexing for multidimensional data arised together with efficient method of document embedding using artificial neural networks, such as \emph{word2-vec} \cite{word2vec} and later deeper networks \cite{dssm}\cite{bert}. Internet became an endless source of data, including web pages, Wikipedia articles, papers, images which form collections with $10^5$ to $10^{10}$ items in each. 
Search for similar items in such collections can no longer be exhaustive. It requires sublinear search time, which automatically mean that we need to use approximate methods.
This problem statement is also known as \emph{approximate nearest neighbour search} (ANNS).
For now industrial applications for large vector datasets utilize three different approaches to building indices.

\emph{Tree based}. Since the invention of search trees such as RB-trees, AVL-trees, B-trees and others, trees were powerful tool to build $\mathcal{O}(\log(|V|))$ indices for numerical data. Quad-trees \cite{quadtrees} and KD-trees \cite{kdtree} were the first attempts to index multidimensional data. Unfortunately their usage is limited to low-dimensional data such as geographical coordinates (2D) or computer graphics (3D).
For higher dimensions and larger datasets these data structures suffer from curse of dimensionality. E.g. for a $10^9$ items KD-tree will utilize only 
$\log_2(10^9)\approx 30$ first dimensions of the vector, while contemporary models produce 100-1000 dimensional vectors \cite{bert}.
To solve the problem with dimensions for tree-based index structures applied to multidimensional data, authors of \cite{annoy} apply random projections and multiple trees. Keeping hundreds of trees can achieve high ANNS accuracy with comparatively small practical search time. But as each tree consumes memory proportional to dataset size, users should pay significant memory for accuracy.

\emph{Inverted index based}. Inverted index was always efficiently used for indexing of texts, as it utilized the properties of human language. We have discrete and limited enough vocabularies of natural languages to prepare in-memory or on-drive index structures with the help of search trees and hash-tables. Multidimensional vector data is continuous, that is why authors of contemporary billion-scale indices use different discretization approaches, such as vector quantization and vector clustering, to prepare "vocabularies" \cite{InvertedMultiIndex}\cite{RevisitedInvertedIndex}. These structures show very promising speed and ANNS accuracy, but still require significant additional memory, which remains them the only place to live -- datacenters. In \cite{InvertedMultiIndex} authors also discuss improvement to inverted indices based on k-means clustering. They propose product quantization (PQ) to split the space in a different way which significantly increases cell density and thus improves ANNS recall. Good thing is that this is still fast and scalable for 1 billion of 128-dimensional vectors \cite{bigann}.

\emph{Proximity graph based}. Proximity graph is a graph in which two vertices are connected by an edge if and only if (or with higher probability if) the vertices satisfy particular geometric requirements, e.g. they are close in metrics space.
Building a proximity graphs for vector datasets can be understood as building a road networks, which allows to travel from any graph node in a direction of desired target (e.g. search query) by following greedy strategy.
Both fast in search and easy in construction, navigable small world (NSW) and hierarchical NSW \cite{hnsw} graphs represent approximation of Delaunay triangulation and show very good results in ANNS accuracy and performance.

\subsection{Instance-based learning}

Instance-based learning is a branch of machine learning which benefit from the fact, that we can keep original dataset as a part of classification or regression model, and make decisions based on real examples. The best known instance-based method is a class of \emph{k-nearest neighbours} algorithms. Still, being able to minimize the size of stored data can be crucial for practical applications. 

In \cite{BhattacharyaGeometricPaper} authors state they combined most promising approaches in instance-based learning into state-of-the-art solutions. Techniques highlighted in the paper are:
\begin{itemize}
    \item use Gabriel graphs as proximity graph for search (which are more sparse compared to Delaunay triagulation), which can be a point of discussion,
    \item Wilson editing \cite{Wilson} is used to improve accuracy and generalization,
    \item Voronoi condensing (thinning) adaptation to Gabriel graphs, i.e. remove all instances, surrounded only by items of the same class,
    \item iterative case filtering \cite{IterativeCaseFiltering} considered another thinning technique and utilizes the same idea -- remove instances from a dataset in case classification of items in their neighbourhood is supported by other instances.
    \item for search speed use Spatial Approximation Sample Hierarchy (SASH) data structure modification for Gabriel graph.
    % http://research.nii.ac.jp/~meh/sash/sashpage.html
\end{itemize}

Authors of \cite{ProximityGraphSurvey} also support Wilson editing \cite{Wilson} technique and instance selection \cite{InstanceSelection}, although they highlight that latter can work normally in general, but fail interesting particular cases. They also propose to use interesting aposteriori metric HVDM \cite{HVDM}, which puts less distance to instances which are classified similarly.

In \cite{ProximityGraphSurvey} authors compare all proximity-based improvements against support-vector machine (SVM) classifier and state that SVM performs statistically better. This comparison is correct by nature, as all instance-based improvements exploit the idea that class border stores important information, and SVM is a border-estimation method.
On the other hand, instance based methods show enormously better scalability and easily deployed in distributed environments. While faster SVM implementations like LaSVM \cite{LaSVM} are used for million-scale dataset, contemporary tasks utilize 3-4-orders bigger datasets. Any way, we would like to emphasize the idea, that the most important information to build a classifier is stored in near-border instances (support vectors), and our work will strongly benefit from this fact.

%% Read This \cite{BhattacharyaGeometricPaper}

\subsection{Proximity graphs}

There are multiple types of exact and approximate proximity graphs graphs, including: minimum spanning tree (MST), relative neighborhood graphs (RNG), Gabriel graphs, Delaunay triangulations.
Index structures utilize the fact, that document vector representation is usually obtained in some latent metric space (or manifold), thus we use cosine metric or Euclidean distance to express similarity.

In \cite{ProximityGraphSurvey} authors overview proximity graph-based improvements to instance-based learning algorithms (i.e. k-NN). These improvements are considered as good resource savers without significant quality loss. Authors state, that Gabriel graphs are very good in low dimensional spaces, but they become very dense is highly dimensional cases together with Delaunay graphs. Thus, minimal spanning trees and relative neighbour graphs are considered as sparser replacements. MSTs failed this test and were considered too sparse, thus not capturing class border correctly.

Among proximity graphs there is a group of data structures based on the idea of \emph{small world graphs}. Major benefit of small worlds (SW) networks \cite{swg} compared to other graphs is that together with edges connecting tight neighborhoods (compare with roads), they also store distant edges (compare with flights). Existence of such edges guarantee $\mathbb{E}(log(|V|))$ expected shortest path between arbitrary pair of vertices. In navigable small world (NSW) \cite{nsw} and hierarchical NSW \cite{hnsw} papers authors combine this property with metric space, introducing greedy-like algorithm to traverse the graph. Authors claim that their data structure approximate Delaunay triangulations in high dimensions, and propose a novel method of constructing SW graphs in metric space, which ends up in $\mathcal{O}(|V|log(|V|))$ construction time complexity and $D*|V|$ memory overhead, where $D$ represent number of dimensions in data.
These data structures are promising for our research in two aspects. Firstly, they approximate the most dense proximity graphs, and any other proximity graph can be created by removing edges. And secondly, proposed procedure of creating such graphs is very efficient in large scale applications.

In our paper we address instance-based learning approach. We benefit from NSW procedure of graph creation, and apply the method inspired by Wilson editing to improve the robustness of our method.