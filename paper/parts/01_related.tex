\section{Relates works}\label{sec:OVERVIEW}

In this section we will focus on there major areas related to our research. Firstly we will overview, how a problem of large scale multidimensional indexing is solved in industry right now. Then we will address a problem of instance-based learning and which techniques can be used to minimize dataset size, but preserve classification accuracy on the same level. Finally, we will discuss proximity graphs and their application to instance-based learning.

\subsection{Large-scale index structures}

Problem of large scale indexing for multidimensional data arised together with efficient method of document embedding using artificial neural networks, such as \emph{word2-vec} \cite{word2vec} and later deeper networks [TODO: transformers, elmo, GPT, MS]. Internet became an endless source of data, including web pages, Wikipedia articles, papers, images which form collections with $10^5$ to $10^{10}$ items in each. 
Search for similar items in such collections can no longer be exhaustive. It requires sublinear search time, which automatically mean that we need to use approximate methods.
This problem statement is also known as \emph{approximate nearest neighbour search} (ANNS).
For now industrial applications for large vector datasets utilize three different approaches to building indices.

\emph{Tree based}. Since the invention of search trees such as RB-trees, AVL-trees, B-trees and others, trees were powerful tool to build $\mathcal{O}(\log(n))$ indices for numerical data. Quad-trees [TODO] and KD-trees [TODO] were the first attempts to index multidimensional data. Unfortunately their usage is limited to low-dimensional data such as geographical coordinates (2D) or computer graphics (3D).
For higher dimensions and larger datasets these data structures suffer from curse of dimensionality. E.g. for a $10^9$ items KD-tree will utilize only $\log_2(10^9)=9\log_2(10)\approx 30$ first dimensions of the vector, while contemporary models produce 100-1000 dimensional vectors [BERT][GPT][RESNET].
To solve the problem with dimensions for tree-based index structures applied to multidimensional data, authors of \cite{annoy} apply random projections and multiple trees. Keeping hundreds of trees can guarantee high ANNS accuracy with comparatively small practical search time.

\emph{Inverted index based}. Inverted index was always efficiently used for indexing of texts, as it utilized the properties of human language. We have discrete and limited enough vocabularies of natural languages to prepare in-memory or on-drive index structures with the help of search trees and hash-tables. Multidimensional vector data is continuous, that is why authors of contemporary billion-scale indices use different discretization approaches, such as vector quantization and vector clustering, to prepare "vocabularies" \cite{InvertedMultiIndex}\cite{RevisitedInvertedIndex}. These structures show very promissing speed and ANNS accuracy, but still require significant additional memory, which remains them the only place to live -- datacenters. In \cite{InvertedMultiIndex} authors discuss improvement to inverted indices based on k-means clustering. They propose product quantization (PQ) to split the space in a different way which significantly increases cell density and thus improves ANNS recall. Good thing is that this is still fast and scalable for 1 billion of 128-dimensional vectors [BIGANN].They also cite Tiny Images dataset: 80M of 384-dim images, which is not available for researched any more.

\emph{Proximity graph based}. Proximity graph is a graph in which two vertices are connected by an edge if and only if (or with higher probability if) the vertices satisfy particular geometric requirements, e.g. they are close in metrics space.
Building a proximity graphs for vector datasets can be understood as building a road networks, which allows to travel from any graph node in a direction of desired target (e.g. search query) by following greedy strategy.
Both fast in search and easy in construction, navigable small world (NSW) and hierarchical NSW \cite{hnsw} graphs represent approximation of Delaunay triangulation and show very good results in ANNS accuracy and performance.

\subsection{Instance-based learning}

Instance-based learning is a branch of machine learning which benefit from the fact, that we can keep original dataset as a part of classification or regression model, and make decisions based on real examples. The best known instance-based method is a class of \emph{k-nearest neighbours} algorithms. Still, being able to minimize the size of stored data can be crucial for practical applications. 

In \cite{BhattacharyaGeometricDR} authors state they combined most promising approaches in instance-based learning into state-of-the-art solutions. Techniques highlighted in the paper are:
\begin{itemize}
    \item use Gabriel graphs as proximity graph for search (which are more sparse compared to Delaunay triagulation), which can be a point of discussion,
    \item Wilson editing \cite{Wilson} is used to improve accuracy and generalization,
    \item Voronoi condensing (thinning) adaptation to Gabriel graphs, i.e. remove all instances, surrounded only by items of the same class,
    \item iterative case filtering \cite{IterativeCaseFiltering} considered another thinning technique and utilizes the same idea -- remove instances from a dataset in case classification of items in their neighbourhood is supported by other instances.
    \item for search speed use Spatial Approximation Sample Hierarchy (SASH) data structure modification for Gabriel graph.
    % http://research.nii.ac.jp/~meh/sash/sashpage.html
\end{itemize}

Authors of \cite{ProximityGraphSurvey} also support Wilson editing \cite{Wilson} technique and instance selection \cite{InstanceSelection}, although they highlight that latter can work normally in general, but fail interesting particular cases. They also propose to use interesting aposteriori metric HVDM \cite{HVDM}, which puts less distance to instances which are classified similarly.

Without any doubt Support Vector Machine (SVM) method, which can be also categorized as instance-based, should be considered as a main competitor to a proposed approach. In \cite{ProximityGraphSurvey} authors compare all proximity-based improvements against SVM and state that TODO WHAT THE STATE.

TODO: Read This \cite{BhattacharyaGeometricPaper}

\subsection{Proximity graphs}

There are multiple types of exact and approximate proximity graphs graphs, including: minimum spanning tree (MST), relative neighborhood graphs (RNG), Gabriel graphs, Delaunay triangulations.
Index structures utilize the fact, that document vector representation is usually obtained in some latent metric space (or manifold), thus we use cosine metric or Euclidean distance to express similarity.
In \cite{ProximityGraphSurvey} authors overview proximity graph-based improvements to instance-based learning algorithms (i.e. k-NN). These improvements are considered as good resource savers without significant quality loss. Authors state, that Gabriel graphs are very good in low dimensional spaces, but they become very dense is highly dimensional cases together with Delaunay graphs. Thus, minimal spanning trees and relative neighbour graphs are considered as sparser replacements. MSTs failed this test and were considered too sparse, thus not capturing class border correctly.

TODO more about NSW/HNSW.